# Introduction to the intended function of message.reply_to_message and what's expected in WebChat

When an LLM is responding to a user message, the system calls the message.as_llm_chat first to retrive the chat in an LLM compatible format. By default, message.as_llm_chat doesn't retrive all messages in the chat but instead it traverses throug their reply_to_message to retrive only relevant messages. This provides a chan on messages connected with that field, and the chain ends when a message's reply_to_message is null. In WebChat, when the user sends a message in a chat, currently it gets created with no reply_to_message even if they are actually following up to a previous assistance response. This is wrong behaviour because in order not to break the context chain, the reply_to_message for the user message being created must be set to the 

# Chat branching

Currently, a chat is just a collinear sequence of messages where if the user sent a message and then figured out that they could've rephrased it better, there's no way back. They can only follow up with another message but the previouse message would still be included in the context used by the LLM, which could confuse the LLM. That's why in ChatGPT, Open AI introduced a feature where usere can edit their message and the AI will not respond again but instead of the AI seeing the user's previous version of the message, it would see the new version, and the previouse version and the AI response to it are still available and the user can still navigate back to that previouse version throug some back and forth arrows in the message. We can implement the exact same mechanism here in unicom utilising the message.reply_to_message field, no changes to the data models are needed but rather just changes to the UI and the logic through which chat messages are loaded as well as the logic through which user messages are sent.
